{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from inception import inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv3x3(in_, out):\n",
    "    return nn.Conv2d(in_, out, 3, padding=1)\n",
    "\n",
    "\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_, out):\n",
    "        super().__init__()\n",
    "        self.conv = conv3x3(in_, out)\n",
    "        self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class NoOperation(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvRelu(in_channels, middle_channels),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class DecoderBlockV2(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels, is_deconv=True,\n",
    "                output_padding=0):\n",
    "        super(DecoderBlockV2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1, output_padding=output_padding),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Interpolate(nn.Module):\n",
    "    def __init__(self, mode='nearest', scale_factor=2,\n",
    "                 align_corners=False, output_padding=0):\n",
    "        super(Interpolate, self).__init__()\n",
    "        self.interp = nn.functional.interpolate\n",
    "        self.mode = mode\n",
    "        self.scale_factor = scale_factor\n",
    "        self.align_corners = align_corners\n",
    "        self.pad = output_padding\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.mode in ['linear','bilinear','trilinear']:\n",
    "            x = self.interp(x, mode=self.mode,\n",
    "                            scale_factor=self.scale_factor,\n",
    "                            align_corners=self.align_corners)\n",
    "        else:\n",
    "            x = self.interp(x, mode=self.mode,\n",
    "                            scale_factor=self.scale_factor)\n",
    "            \n",
    "        if self.pad > 0:\n",
    "            x = nn.ZeroPad2d((0, self.pad, 0, self.pad))(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlockV3(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels,\n",
    "                 is_deconv=True, output_padding=0):\n",
    "        super(DecoderBlockV3, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        if is_deconv:\n",
    "            \"\"\"\n",
    "                Paramaters for Deconvolution were chosen to avoid artifacts, following\n",
    "                link https://distill.pub/2016/deconv-checkerboard/\n",
    "            \"\"\"\n",
    "\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, middle_channels, kernel_size=4, stride=2,\n",
    "                                   padding=1, output_padding=output_padding),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                Interpolate(mode='nearest', scale_factor=2,\n",
    "                           output_padding=output_padding),\n",
    "                # nn.Upsample(scale_factor=2, mode='bilinear'),\n",
    "                ConvRelu(in_channels, middle_channels),\n",
    "                ConvRelu(middle_channels, out_channels),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "\n",
    "class InceptionResnetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, num_filters=32, \n",
    "                 pretrained=True, is_deconv=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(3)\n",
    "            \n",
    "        self.encoder = pretrainedmodels.__dict__['inceptionresnetv2'](num_classes=1000,\n",
    "                                              pretrained='imagenet')\n",
    "        \n",
    "#         self.encoder = torchvision.models.inceptionresnet_v2(pretrained=True)\n",
    "        \n",
    "#         self.encoder = inception_v3(pretrained=True)\n",
    "        \n",
    "        # remove final two layers\n",
    "        self.encoder = nn.Sequential(*list(self.encoder.children())[:-2])\n",
    "        \n",
    "        self.conv = nn.Conv2d(1536, 32, 1)\n",
    "        self.act1 = nn.ReLU(inplace=True)\n",
    "        self.csize = 32 * 14 * 14\n",
    "        self.do1a = nn.Dropout(0.5)\n",
    "        self.fc1a = nn.Linear(self.csize, 1024)\n",
    "        self.act2a = nn.ReLU(inplace=True)\n",
    "        self.do2a = nn.Dropout(0.5)\n",
    "        self.fc2a = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # set to True for debugging\n",
    "        print_sizes = True\n",
    "        if print_sizes: \n",
    "            print('')\n",
    "            print('x',x.shape)\n",
    "        \n",
    "        # print layer dictionary\n",
    "        # print(self.encoder.features)\n",
    "        \n",
    "        x = self.bn(x)\n",
    "        if print_sizes: print('bn',x.shape)\n",
    "        \n",
    "#         m = self.encoder._modules\n",
    "#         layer_names = list(m.keys())\n",
    "#         mx = {}\n",
    "#         for i,f in enumerate(m):\n",
    "#             if layer_names[i] == 'AuxLogits': continue\n",
    "#             elif layer_names[i] == 'fc': break\n",
    "#             x = m[f](x)\n",
    "#             mx[layer_names[i]] = x\n",
    "#             if print_sizes:\n",
    "#                 if isinstance(x,tuple):\n",
    "#                     print(i,layer_names[i],x[0].size(),x[1].size())\n",
    "#                 else:\n",
    "#                     print(i,layer_names[i],x.size())\n",
    "           \n",
    "        x = self.encoder(x)\n",
    "        if print_sizes: print('encoder',x.shape)\n",
    "            \n",
    "        x = self.conv(x)\n",
    "        if print_sizes: print('conv',x.shape)\n",
    "        x = self.act1(x) \n",
    "        x = x.view(-1, self.csize)\n",
    "        if print_sizes: print('view',x.shape)\n",
    "        x = self.do1a(x)\n",
    "        x = self.fc1a(x)\n",
    "        if print_sizes: print('fc1a',x.shape)\n",
    "        x = self.act2a(x)\n",
    "        x = self.do2a(x) \n",
    "        x = self.fc2a(x)\n",
    "        if print_sizes: print('fc2a',x.shape)\n",
    "\n",
    "        return x\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
